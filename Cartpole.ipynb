{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "import gym\n",
    "import copy\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Demonstration\n",
    "# env = gym.envs.make(\"CartPole-v1\")\n",
    "# def get_screen():\n",
    "#     ''' Extract one step of the simulation.'''\n",
    "#     screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "#     screen = np.ascontiguousarray(screen, dtype=np.float32) / 255.\n",
    "#     return torch.from_numpy(screen)\n",
    "# # Speify the number of simulation steps\n",
    "# num_steps = 2\n",
    "# # Show several steps\n",
    "# for i in range(num_steps):\n",
    "#     clear_output(wait=True)\n",
    "#     env.reset()\n",
    "#     plt.figure()\n",
    "#     plt.imshow(get_screen().cpu().permute(1, 2, 0).numpy(),\n",
    "#                interpolation='none')\n",
    "#     plt.title('CartPole-v0 Environment')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL():\n",
    "    ''' Deep Q Neural Network class. '''\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):\n",
    "            self.criterion = torch.nn.MSELoss()\n",
    "            self.model = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(state_dim, hidden_dim),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim, hidden_dim*2),\n",
    "                            torch.nn.LeakyReLU(),\n",
    "                            torch.nn.Linear(hidden_dim*2, action_dim)\n",
    "                    )\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "def update(self, state, y):\n",
    "        \"\"\"Update the weights of the network given a training sample. \"\"\"\n",
    "        y_pred = self.model(torch.Tensor(state))\n",
    "        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "def predict(self, state):\n",
    "        \"\"\" Compute Q values for all actions using the DQL. \"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, model, episodes, gamma=0.9, \n",
    "               epsilon=0.3, eps_decay=0.99,\n",
    "               replay=False, replay_size=20, \n",
    "               title = 'DQL', double=False, \n",
    "               n_update=10, soft=False, verbose=True):\n",
    "    \"\"\"Deep Q Learning algorithm using the DQN. \"\"\"\n",
    "    final = []\n",
    "    memory = []\n",
    "    episode_i=0\n",
    "    sum_total_replay_time=0\n",
    "    for episode in range(episodes):\n",
    "        episode_i+=1\n",
    "        if double and not soft:\n",
    "            # Update target network every n_update steps\n",
    "            if episode % n_update == 0:\n",
    "                model.target_update()\n",
    "        if double and soft:\n",
    "            model.target_update()\n",
    "        \n",
    "        # Reset state\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Implement greedy search policy to explore the state space\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "            \n",
    "            # Take action and add reward to total\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Update total and memory\n",
    "            total += reward\n",
    "            memory.append((state, action, next_state, reward, done))\n",
    "            q_values = model.predict(state).tolist()\n",
    "             \n",
    "            if done:\n",
    "                if not replay:\n",
    "                    q_values[action] = reward\n",
    "                    # Update network weights\n",
    "                    model.update(state, q_values)\n",
    "                break\n",
    "\n",
    "            if replay:\n",
    "                t0=time.time()\n",
    "                # Update network weights using replay memory\n",
    "                model.replay(memory, replay_size, gamma)\n",
    "                t1=time.time()\n",
    "                sum_total_replay_time+=(t1-t0)\n",
    "            else: \n",
    "                # Update network weights using the last step only\n",
    "                q_values_next = model.predict(next_state)\n",
    "                q_values[action] = reward + gamma * torch.max(q_values_next).item()\n",
    "                model.update(state, q_values)\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        # Update epsilon\n",
    "        epsilon = max(epsilon * eps_decay, 0.01)\n",
    "        final.append(total)\n",
    "        plot_res(final, title)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"episode: {}, total reward: {}\".format(episode_i, total))\n",
    "            if replay:\n",
    "                print(\"Average replay time:\", sum_total_replay_time/episode_i)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(195, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(195, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states\n",
    "n_state = env.observation_space.shape[0]\n",
    "# Number of actions\n",
    "n_action = env.action_space.n\n",
    "# Number of episodes\n",
    "episodes = 150\n",
    "# Number of hidden nodes in the DQN\n",
    "n_hidden = 50\n",
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
